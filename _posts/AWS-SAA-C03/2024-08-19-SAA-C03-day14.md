---
title: "SAA-C03 Day 14 | Data & Analytics"
date: 2024-08-19 00:00:00
categories: [AWS, SAA-C03]
tags: [Analytics]
---


### Athena
- Serverless query service to analyze data stored in S3
- uses SQL query
- pay for per TB data scanned
- commonly used with quick sight dashboard
- used for analyze logs
- Federated Query: It can allow u to run SQL across data store in relational non relational object and custom data
  - In services like Amazon Athena, configure data source connectors that link to your external databases, data lakes, or other data sources. These connectors allow Athena to access and query the data.
  - u need set a S3 to store result
#### Performance Improvenment
1. use **columnar data** for cost saving
   - You can convert your data to a columnar format like Parquet or ORC using AWS Glue, Apache Spark, or other ETL tools. For example, you might extract data from a database, transform it into Parquet format, and then store it in S3.
2. compress data for smaller retrievals(some format)
3. partition dataset in S3 for easy querying (based on year date..)

---

### RedShift
- OLAP（Analytic）
- Columnar storage of data (instead of row based) & parallel query engine
- can perform SQL
- vs Athena: faster queries / joins / aggregations thanks to indexes
- Redshift is a cluster with a leader Node


#### Redshift Snapshots & DR
- Redshift has “Multi-AZ” mode for some clusters
- Snapshots are incremental (only what has changed is saved)
- You can restore a snapshot into a new cluster
- Snapshots are point-in-time backups of a cluster, stored internally in S3
- You can configure Amazon Redshift to automatically copy snapshots (automated or manual) of a cluster to another AWS Region


#### Loading Data to Redshift
1. Kinesis DataFirehose to S3 -》 S3 copy to Redshift
2. EC2 JDBC driver to redshift


#### Redshift Spectrum
1. allows you to run SQL queries directly against exabytes of data in Amazon S3 without having to load the data into Redshift tables
---

### OpenSearch （successor to AMason ELasting Search）
- In DynamoDB, queries only exist by primary key or indexes... 
- With OpenSearch, you can search any field, even partially matches
- It’s common to use OpenSearch as a complement to another database

#### load data to opensearch via kinesis
1. Kinesis Data stream -> lambda -> opensearch
2. Kinesis Data stream -> Kinesis Firehose -> lambda(transformation) -> opensearch
---

### Amazon EMR
- Elastic MapReduce
- EMR helps creating Hadoop clusters (Big Data) to analyze and process vast amount of data
- Use cases: data processing, machine learning, web indexing, big data

#### Node type (cluster of ec2 nodes)
- Master Node: Manage the cluster, coordinate, manage health – long running 
- Core Node: Run tasks and store data – long running
- Task Node (optional): Just to run tasks – usually Spot

---
### Amazon QuickSight
- Serverless machine learning-powered business intelligence service to create interactive dashboards
- Apply on data sources
- Use cases: Business analytics,  Building visualizations ,Perform ad-hoc analysis , Get business insights using data

#### Dashboard and Analysis
- Define Users (standard versions) and Groups (enterprise version)
  - These users & groups only exist within QuickSight, not IAM !!
- A dashboard...
  - is a read-only snapshot of an analysis that you can share
  - preserves the configuration of the analysis (filtering, parameters, controls, sort)
- You can share the analysis or the dashboard with Users or Groups - To share a dashboard, you must first publish it
- Users who see the dashboard can also see the underlying data
  
--- 
### AWS GLUE
- Managed extract, transform, and load (ETL) service 
- Useful to prepare and transform data for analytics
- Fully serverless service

#### Convert data into Parquet format for Athena fast performance



#### Good to know
Glue Job Bookmarks: prevent re-processing old data
- Glue Elastic Views:
- Combine and replicate data across multiple data stores using SQL
- No custom code, Glue monitors for changes in the source data, serverless - Leverages a “virtual table” (materialized view)
- Glue DataBrew: clean and normalize data using pre-built transformation
- Glue Studio: new GUI to create, run and monitor ETL jobs in Glue
- Glue Streaming ETL (built on Apache Spark Structured Streaming): compatible with Kinesis Data Streaming, Kafka, MSK (managed Kafka)


---

### AWS Lake Formation
data lake is a **centralized** repository that allows you to store all your structured and unstructured data at any scale. With Lake Formation, you can ingest, catalog, cleanse, transform, and secure your data in a way that makes it easy to analyze and gain insights.

#### 1. Out-of-the-Box Source Blueprints:

- **Amazon S3:** Easily ingest data stored in S3 buckets into your data lake.
- **Amazon RDS:** Automatically connect and ingest data from Amazon Relational Database Service (RDS) instances.
- **Relational Databases:** Support for ingesting data from various relational databases such as MySQL, PostgreSQL, and SQL Server.
- **NoSQL Databases:** Ingest data from NoSQL databases, providing flexibility for different types of data storage systems.
- 
- **Pre-Built Blueprints:** Lake Formation provides pre-built blueprints to streamline the process of setting up data ingestion workflows. These blueprints simplify tasks like loading logs, copying data from databases, and converting data formats.

### 2. Fine-Grained Access Control:

- **Fine-Grained Policies:** Lake Formation allows you to define and enforce fine-grained access control policies, ensuring that only authorized users can access specific rows or columns within your datasets.

- **Central Management:** Manage access control from a central location, simplifying the administration of security policies across your data lake.

- **Application Security:** Ensure that your applications only access the data they need, protecting sensitive information and maintaining compliance with data privacy regulations.


---

### Kinesis Data Analytics for SQL applications
Amazon Kinesis can be used in a setup where data is processed and then sent back to another Kinesis stream or Kinesis Firehose for further processing or storage. Here's how this works:


--- 

### Big Data Ingestion Pipeline
